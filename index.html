<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="` - General Purpose Multimodal Knowledge Retriever">
  <meta property="og:title" content="PreFLMR" />
  <meta property="og:description" content="PreFLMR - General Purpose Multimodal Knowledge Retriever" />
  <meta property="og:url" content="https://jingbiaomei.github.io/PreFLMR_Page/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="PreFLMR">
  <meta name="twitter:description" content="PreFLMR - General Purpose Multimodal Knowledge Retriever">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords"
    content="PreFLMR, Multimodal, Retriever, Multimodal Retriever, Knowledge Retriever, M2KR, KBVQA, LMM, LLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.16.0/gradio.js"></script>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-1WTVVMF6GQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-1WTVVMF6GQ');
  </script>

</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">PreFLMR</h1>
            <h1 class="subtitle is-2 publication-subtitle">Scaling Up Fine-Grained Late-Interaction Multi-modal
              Retrievers</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://linweizhedragon.github.io/" target="_blank">Weizhe Lin</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://jingbiao.me/" target="_blank">Jingbiao Mei</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://www.jinghong-chen.net/" target="_blank">Jinghong Chen</a><sup>*</sup>,</span>
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/view/bill-byrne/home" target="_blank">Bill
                  Byrne</a><sup></sup></span>
              </span>

            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">University of Cambridge<br></span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.08327" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link 
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                </span>-->

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/linweizhedragon/retrieval-augmented-visual-question-answering"
                    target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Demo link -->
                <span class="link-block">
                  <a href="https://u60544-b8d4-53eaa55d.westx.seetacloud.com:8443/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <span class="fa fa-images"></span>
                    </span>
                    <span>Demo</span>
                  </a>
                </span>

                <!-- HF link Model -->
                <span class="link-block">
                  <a href="https://huggingface.co/LinWeizheDragon/PreFLMR_ViT-G" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      🤗
                    </span>
                    <span>Model</span>
                  </a>
                </span>

                <!-- HF link Dataset-->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/BByrneLab/multi_task_multi_modal_knowledge_retrieval_benchmark_M2KR" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <span class="fa fa-database"></span>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>

                <!-- HF link Dataset-->
                <span class="link-block">
                  <a href="https://www.jinghong-chen.net/preflmr-sota-open-sourced-multi/?ref=Project_page" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <span class="fa fa-blog"></span>
                    </span>
                    <span>Blog</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link 
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
                </span>-->

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video 
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">-->
  <!-- Your video here 
          <source src="static/videos/banner_video.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat
          pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.
        </h2>
      </div>
    </div>
  </section>-->
  <!-- End teaser video -->

  <!-- Gradio
  <section class="section">
    <div class="container is-max-desktop">
      <gradio-app src="http://region-3.seetacloud.com:38703"></gradio-app>
    </div>
  </section> -->

  <!-- Paper abstract -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Large Multimodal Models (LMMs) excel in natural language and visual understanding but are challenged by
              exacting tasks such as Knowledge-based Visual Question Answering (KB-VQA) which involve the retrieval of
              relevant information from document collections to use in shaping answers to questions.
              We present an extensive training and evaluation framework, M2KR, for KB-VQA.
              M2KR contains a collection of vision and language tasks which we have incorporated into a single suite of
              benchmark tasks for training and evaluating general-purpose multi-modal retrievers.
              We use M2KR to develop PreFLMR, a pre-trained version of the recently developed Fine-grained
              Late-interaction Multi-modal Retriever (<a href="https://arxiv.org/abs/2309.17133"
                target="_blank">FLMR</a>) approach to KB-VQA, and we report new state-of-the-art results across a range
              of tasks.
              We also present investigations into the scaling behaviors of PreFLMR intended to be useful in future
              developments in general-purpose multi-modal retrievers.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->
  <!-- Image -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns">
        <div class="column is-five-sixths">
          <h2 class="title is-3 has-text-centered">Model</h2>
          <div class="content">
            <p>
              (1) the text query consists of an instruction and a question, which is encoded by a text encoder;
              (2) at the output of the vision encoder, a mapping network consisting of Multi-Layer Perceptrons (MLP)
              converts the `[CLS]' token representations into the same embedding space as the text encoder;
              (3) the transformer blocks take in the patch image embeddings from the penultimate layer of the vision
              encoder and attend to the text features by cross-attention;
              (4) a text encoder encodes documents in the knowledge base;
              (5) the scores between queries and documents are computed based on late-interaction, allowing each query
              token to interact with all document token embeddings.
            </p>
            <img src="static/images/Architecture.jpg" alt="PreFLMR Architecture" />
          </div>
          <div class="item">

          </div>
        </div>
      </div>
  </section>


  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns">
        <div class="column is-three-fourths">
          <h2 class="title is-3 has-text-centered">Retrieval Performance</h2>
          <div class="content">
            <p>
              We show PreFLMR's performance when incorporating vision encoders with different scales with base sized
              text encoder. All variants of PreFLMR shows competitive performance on all nine benchmark retrieval tasks
              in the M2KR.
            </p>
            <img src="static/images/radar_plot.jpg" alt="Retrieval Performance" />
          </div>
          <div class="item">
            <p>
              For the evaluation metrics, WIT uses Recall@10, IGLUE uses Recall@1, all the rest datasets use Recall@5.
              The scale of the plot is adjusted for better visualization. The best and worst numbers of each task are
              annotated.
            </p>
          </div>
        </div>
      </div>
  </section>

  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns">
        <div class="column is-five-sixths">
          <h2 class="title is-3 has-text-centered">KB-VQA Performance</h2>
          <div class="content">
            <p> We show downstream KB-VQA performance when <a
                href="https://github.com/linweizhedragon/retrieval-augmented-visual-question-answering"
                target="_blank">RA-VQAv2</a> is equipped with PreFLMR and finetuned on the target VQA task. PaLM-E,
              PALI-X, and PaLM-B are large multi-modal models with 562B, 55B, and 1T parameters, respectively. The E-VQA
              SOTA uses Lens, the Google API for image retrieval. After incorporating PreFLMR, RA-VQAV2 achieves
              competitive performance while using much smaller language model.</p>
            <table>

              <thead>
                <tr>
                  <th>Model</th>
                  <th>OKVQA</th>
                  <th>Infoseek</th>
                  <th>E-VQA</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>SOTA</td>
                  <td>66.10</td>
                  <td>21.80</td>
                  <td>48.80</td>
                </tr>
                <tr>
                  <td style="padding-left: 20px;"><i>SOTA model</i></td>
                  <td>PaLM-E</td>
                  <td>PALI-X</td>
                  <td>PaLM-B + Lens</td>
                </tr>
                <tr>
                  <td>RA-VQAv2 w/ PreFLMR</td>
                  <td>61.88</td>
                  <td>30.65</td>
                  <td>54.45</td>
                </tr>
                <tr>
                  <td style="padding-left: 20px;"><i>w/o retrieval</i></td>
                  <td>55.44</td>
                  <td>21.78</td>
                  <td>19.80</td>
                </tr>
              </tbody>
            </table>



          </div>
          <div class="item">
          </div>
        </div>
      </div>
  </section>



  <!-- Image carousel 
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">

            <img src="static/images/carousel1.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              First image description.
            </h2>
          </div>
          <div class="item">

            <img src="static/images/carousel2.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Second image description.
            </h2>
          </div>
          <div class="item">

            <img src="static/images/carousel3.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Third image description.
            </h2>
          </div>
          <div class="item">

            <img src="static/images/carousel4.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Fourth image description.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>-->
  <!-- End image carousel -->




  <!-- Youtube video 
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">

        <h2 class="title is-3">Video Presentation</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">

            <div class="publication-video">

              <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media"
                allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>-->
  <!-- End youtube video -->


  <!-- Video carousel 
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Another Carousel</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-video1">
            <video poster="" id="video1" autoplay controls muted loop height="100%">

              <source src="static/videos/carousel1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video2">
            <video poster="" id="video2" autoplay controls muted loop height="100%">

              <source src="static/videos/carousel2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video3">
            <video poster="" id="video3" autoplay controls muted loop height="100%">\

              <source src="static/videos/carousel3.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>-->
  <!-- End video carousel -->






  <!-- Paper poster
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
        </iframe>

      </div>
    </div>
  </section> -->
  <!--End paper poster -->

  <section class="section">

    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3"> Examples with PreFLMR as Knowledge Retriever</h2>


      </div>

    </div>
    <div class="container is-max-desktop">
      <div class="columns is-centered is-five-sixths">
        <div class="column">
          <p> We demonstrate three types of usage of PreFLMR Knowledge Retriever: Image + Instruction + Question to
            retrieve Documents; Image + Instruction to retrieve Documents; Instruction + Question to retrieve Documents.
          </p>

        </div>
      </div>
    </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-five-sixths">
          <h2 class="title is-5">Image + Instruction + Question -> Documents</a></h2>
        </div>
      </div>
      <!-- Convesation 1 -->
      <div class="container mt-5 is-max-desktop">
        <!-- Selection -->
        <div class="form-row" style="justify-content: flex-end;">
          <div class="form-group col-md-1">
            <div class="col-md-2" style="width: 100%"><label>&nbsp;</label></div>
            <div class="btn-group" role="group" aria-label="Left and Right Controller"
              style="width: 100%;align-items: flex-end;justify-content: right;flex-direction: row;display: flex;">
              <button type="button" class="form-control btn btn-primary" id="prev-itt"><i
                  class="material-icons">keyboard_arrow_left</i></button>
              <button type="button" class="form-control btn btn-primary" id="next-itt"><i
                  class="material-icons">keyboard_arrow_right</i></button>

            </div>
          </div>
        </div>
        <!-- Question Card -->
        <div style="display: flex; justify-content: center; align-items: center;">
          <div class="card mb-4" style="width: 100%; display: flex; align-items: center;">
            <!-- <p><b>Description:</b> Monalisa is a famous painting by Leonardo da Vinci. </p> -->

            <div class="modal-card-body" id="selected-question" style="display: flex; height: 80vh;">
              <div id="chat-history-itt" class="chat-history"></div>

            </div>
          </div>
        </div>
      </div>

  </section>

  <section class="section">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-5">Image + Instruction -> Documents</a></h2>
      </div>
    </div>
    <!-- Convesation 2 -->
    <div class="container mt-5 is-max-desktop">
      <!-- Selection -->
      <div class="form-row" style="justify-content: flex-end;">
        <div class="form-group col-md-1">
          <div class="col-md-2" style="width: 100%"><label>&nbsp;</label></div>
          <div class="btn-group" role="group" aria-label="Left and Right Controller"
            style="width: 100%;align-items: flex-end;justify-content: right;flex-direction: row;display: flex;">
            <button type="button" class="form-control btn btn-primary" id="prev-it"><i
                class="material-icons">keyboard_arrow_left</i></button>
            <button type="button" class="form-control btn btn-primary" id="next-it"><i
                class="material-icons">keyboard_arrow_right</i></button>

          </div>
        </div>
      </div>
      <!-- Question Card -->
      <div style="display: flex; justify-content: center; align-items: center;">
        <div class="card mb-4" style="width: 100%; display: flex; align-items: center;">
          <!-- <p><b>Description:</b> Monalisa is a famous painting by Leonardo da Vinci. </p> -->

          <div class="modal-card-body" id="selected-question" style=" height: 80vh;">

            <div id="chat-history-it" class="chat-history"></div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="section">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-5">Instruction + Question -> Documents</a></h2>
      </div>
    </div>
    <!-- Convesation 3 -->


    <div class="container mt-5 is-max-desktop">
      <!-- Selection -->
      <div class="form-row" style="justify-content: flex-end;">
        <div class="form-group col-md-1">
          <div class="col-md-2" style="width: 100%"><label>&nbsp;</label></div>
          <div class="btn-group" role="group" aria-label="Left and Right Controller"
            style="width: 100%;align-items: flex-end;justify-content: right;flex-direction: row;display: flex;">
            <button type="button" class="form-control btn btn-primary" id="prev-tt"><i
                class="material-icons">keyboard_arrow_left</i></button>
            <button type="button" class="form-control btn btn-primary" id="next-tt"><i
                class="material-icons">keyboard_arrow_right</i></button>

          </div>
        </div>
      </div>
      <!-- Question Card -->
      <div style="display: flex; justify-content: center; align-items: center;">
        <div class="card mb-4 content" style="width: 100%; display: flex; align-items: center;">
          <!-- <p><b>Description:</b> Monalisa is a famous painting by Leonardo da Vinci. </p> -->

          <div class="modal-card-body" id="selected-question" style="display: flex; height: 40vh;">

            <div id="chat-history-tt" class="chat-history"></div>

          </div>
        </div>
      </div>

    </div>
  </section>

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code> 
        @inproceedings{
          lin2023finegrained,
          title={Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering},
          author={Weizhe Lin and Jinghong Chen and Jingbiao Mei and Alexandru Coca and Bill Byrne},
          booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
          year={2023},
          url={https://openreview.net/forum?id=IWWWulAX7g}
        }
        
        @article{Lin_Mei_Chen_Byrne_2024, 
        title={PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers}, 
        url={http://arxiv.org/abs/2402.08327}, 
        number={arXiv:2402.08327}, 
        publisher={arXiv}, 
        author={Lin, Weizhe and Mei, Jingbiao and Chen, Jinghong and Byrne, Bill}, 
        year={2024}}
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>This work was supported in part by the AWS Cloud Credit for Research programme. </p>
      <p>This page was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page,
        licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>. Many thanks to the <a
          href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page
          Template</a> </p>
    </div>
  </section>


  <script>
    // Handle message showing
    function createChatRow(sender, text, imageSrc) {
      var article = document.createElement("article");
      article.className = "media"

      var figure = document.createElement("figure");
      figure.className = "media-left";

      var span = document.createElement("span");
      span.className = "icon is-large";

      var icon = document.createElement("i");
      icon.className = "fas fas fa-2x" + (sender === "User" ? " fa-user " : sender === "PreFLMR" ? " fa-robot" : "");

      var media = document.createElement("div");
      media.className = "media-content";

      var content = document.createElement("div");
      content.className = "content";

      var para = document.createElement("p");

      // wrap text in pre tag to preserve whitespace and line breaks
      var pre_text = document.createElement("pre");
      pre_text.style =
        "background-color: white; font-size: 18px; font-family: Arial; padding: 0; margin: 0; white-space: pre-wrap; word-wrap: break-word;";
      var paraText = document.createTextNode(text);
      pre_text.appendChild(paraText);

      var strong = document.createElement("strong");
      strong.innerHTML = sender;
      var br = document.createElement("br");

      para.appendChild(strong);
      para.appendChild(br);
      para.appendChild(pre_text);

      // Add image if imageSrc is provided
      if (imageSrc) {
        var img = document.createElement("img");
        img.src = imageSrc;
        img.style = "max-width: 100%; max-height: 300px;"; // Adjust the style as needed
        para.appendChild(img);
      }

      content.appendChild(para);
      media.appendChild(content);
      span.appendChild(icon);
      figure.appendChild(span);
      if (sender !== "Description") {
        article.appendChild(figure);
      };
      article.appendChild(media);
      return article;
    }

    // Adjusted to accept chatHistoryId parameter
    function addMessageToChatHistory(chatHistoryId, sender, message, imageSrc) {
      const chatHistory = document.querySelector('#' + chatHistoryId);
      const chatRow = createChatRow(sender, message, imageSrc);
      chatHistory.appendChild(chatRow);
      chatHistory.scrollTop = chatHistory.scrollHeight;
    }

    // 
    const conversations_itt = [{
        "description": "Architecture",
        "turns": [
          // ["Description", "Please read the description and answer the question."],
          ["User",
            "Using the provided image, obtain documents that address the subsequent question: What is the capital of the country shown in the image?",
            "./static/images/Demo_1_1.jpg"
          ],
          ["PreFLMR",
            "In the final phases of the Chinese Civil War, the People's Liberation Army seized control of the city peacefully on 31 January 1949 in the course of the Pingjin Campaign. On 1 October that year, Mao Zedong announced the creation of the People's Republic of China from atop Tian'anmen. He restored the name of the city, as the new capital, to Beijing, a decision that had been reached by the Chinese People's Political Consultative Conference just a few days earlier." +
            "In the 1950s, the city began to expand beyond the old walled city and its surrounding neighborhoods, with heavy industries in the west and residential neighborhoods in the north. Many areas of the Beijing city wall were torn down in the 1960s to make way for the construction of the Beijing Subway and the 2nd Ring Road." +
            "During the Cultural Revolution from 1966 to 1976, the Red Guard movement began in Beijing and the city's government fell victim to one of the first purges. By the autumn of 1966, all city schools were shut down and over a million Red Guards from across the country gathered in Beijing for eight rallies in Tian'anmen Square with Mao. In April 1976, a large public gathering of Beijing residents against the Gang of Four and the Cultural Revolution in Tiananmen Square was forcefully suppressed. In October 1976, the Gang was arrested in Zhongnanhai and the Cultural Revolution came to an end. In December 1978, the Third Plenum of the 11th Party Congress in Beijing under the leadership of Deng Xiaoping reversed the verdicts against victims of the Cultural Revolution and instituted the policy of reform and opening up." +
            "Since the early 1980s, the urban area of Beijing has expanded greatly with the completion of the 2nd Ring Road in 1981 and the subsequent addition of the 3rd, 4th, 5th and 6th Ring Roads. According to one 2005 newspaper report, the size of newly developed Beijing was one-and-a-half times larger than before. Wangfujing and Xidan have developed into flourishing shopping districts, while Zhongguancun has become a major center of electronics in China. In recent years, the expansion of Beijing has also brought to the forefront some problems of urbanization, such as heavy traffic, poor air quality, the loss of historic neighborhoods, and a significant influx of migrant workers from less-developed rural areas of the country. Beijing has also been the location of many significant events in recent Chinese history, principally the Tiananmen Square protests of 1989. The city has also hosted major international events, including the 2008 Summer Olympics and the 2015 World Championships in Athletics, and was chosen to host the 2022 Winter Olympics, making it the first city to ever host both Winter and Summer Olympics."
          ]
        ]
      },
      {
        "description": "Natural",
        "turns": [
          ["User",
            "Using the provided image, obtain documents that address the subsequent question: What is the creature in this image?",
            "./static/images/Demo_1_2.jpg"
          ],
          ["PreFLMR",
            "The Ecuadorian hermit crab (Coenobita compressus) also known as the Pacific hermit crab is a species of land hermit crab. It is one of the two land hermit crabs commonly sold in North America as pets, the other being the Caribbean hermit crab (C. clypeatus).​"
          ]
        ]
      },
    ];

    const conversations_it = [{
        "description": "Map",
        "turns": [
          // ["Description", "Please read the description and answer the question."],
          ["User", "Identify the document that is connected to this image.:", "./static/images/Demo_2_1.jpg"],
          ["PreFLMR",
            "title: Peron Peninsula hierarchical section title: Peron Peninsula caption reference description: Satellite image of Shark Bay caption attribution description: Phytoplankton in Bloom. NASA’s Terra satellite captured this image of phytoplankton in bloom in Australia’s Shark Bay on November 6, 2004. content: Peron Peninsula is a long narrow peninsula located in the Shark Bay World Heritage site in Western Australia, at about 25°51' S longitude and 113°30' E latitude. It is some 130 kilometres long, running north-northwesterly, located east of Henri Freycinet Harbour and west of Havre Hamelin and Faure Island. It is the largest of the Shark Bay peninsulas. Significant settlements include Denham and Monkey Mia. An airport is located there. It is the location of former Pastoral leases Peron and Nanga stations. It is the main location of land access to points within the World Heritage site. The northern area contains the Francois Peron National Park. It is surrounded by the Shark Bay Marine Park and its lower southeast part is adjacent to the Hamelin Pool Marine Nature Reserve. The Taillefer Isthmus, the narrowest section of the peninsula, is between Nanga and Goulet Bluff - which has Shell Beach located on the eastern side which lies in the L'Haridon Bight. The northernmost point is Cape Peron which has a namesake further south in the metropolitan Perth coastal area. "
          ]
        ]
      },
      {
        "description": "Building",
        "turns": [
          ["User", "Identify the document that is connected to this image.:", "./static/images/Demo_2_2.jpg"],
          ["PreFLMR",
            "title: Midtown Tower hierarchical section title: Midtown Tower caption reference description: Midtown Tower with Gardenside in the foreground caption attribution description: 日本語: 東京ミッドタウンタワー、東京都港区赤坂9丁目。 content: Midtown Tower is a mixed-use skyscraper in Akasaka, Minato, Tokyo. Completed in 2007, it is the tallest of the six buildings within the Tokyo Midtown complex, at 248.1 meters, and was the tallest office building in Tokyo until 2014.​"
          ]
        ]
      },
    ];

    const conversations_tt = [{
        "description": "petechial",
        "turns": [
          // ["Description", "Please read the description and answer the question."],
          ["User", "Retrieve the document that answers this question.: causes of petechial hemorrhage"],
          ["PreFLMR",
            "Causes and Symptoms. The causes of petechial hemorrhages are injury to the brain and trauma. This type of multiple petechial hemorrhage is not often visible in imaging techniques such as CT and MRI scans. It is visible in case the injury is very severe, and it generally shows up only after few days. "
          ]
        ]
      },
      {
        "description": "transformative",
        "turns": [
          ["User", "Retrieve the document that answers this question.: theory of transformational learning definition"],
          ["PreFLMR",
            "Transformative learning. Transformative learning theory says that the process of perspective transformation has three dimensions: psychological (changes in understanding of the self), convictional (revision of belief systems), and behavioral (changes in lifestyle).​"
          ]
        ]
      },
    ];


    // Adjusted to accept chatHistoryId parameter
    function clearChatHistory(chatHistoryId) {
      const chatHistory = document.querySelector('#' + chatHistoryId);
      chatHistory.innerHTML = "";
    }

    // This is a generalized version of the update function for any conversation set and chat history
    function updateDialogDemo(chatHistoryId, conversations, currentIndex) {
      // Clears the chat history for a specific chat history container
      clearChatHistory(chatHistoryId);

      // Get specific conversations and update the chat history
      const conversation = conversations[currentIndex];
      for (let i = 0; i < conversation.turns.length; i++) {
        if (conversation.turns[i].length == 2) {
          addMessageToChatHistory(chatHistoryId, conversation.turns[i][0], conversation.turns[i][1]);
        } else {
          addMessageToChatHistory(chatHistoryId, conversation.turns[i][0], conversation.turns[i][1], conversation.turns[
            i][2]);
        }
      }
    }
    // Assuming currentIndexes for each conversation set
    let currentIndexITT = 0;
    let currentIndexIT = 0;
    let currentIndexTT = 0;

    // Initialize each chat history with its first conversation
    updateDialogDemo('chat-history-itt', conversations_itt, currentIndexITT);
    updateDialogDemo('chat-history-it', conversations_it, currentIndexIT);
    updateDialogDemo('chat-history-tt', conversations_tt, currentIndexTT);

    document.getElementById('prev-itt').addEventListener('click', () => {
      currentIndexITT = (currentIndexITT - 1 + conversations_itt.length) % conversations_itt.length;
      updateDialogDemo('chat-history-itt', conversations_itt, currentIndexITT);
    });


    // Example event listeners for navigating conversations in the ITT set
    document.getElementById('next-itt').addEventListener('click', () => {
      currentIndexITT = (currentIndexITT + 1) % conversations_itt.length;
      updateDialogDemo('chat-history-itt', conversations_itt, currentIndexITT);
    });
    // Event listeners for IT conversation navigation
    document.getElementById('prev-it').addEventListener('click', () => {
      currentIndexIT = (currentIndexIT - 1 + conversations_it.length) % conversations_it.length;
      updateDialogDemo('chat-history-it', conversations_it, currentIndexIT);
    });

    document.getElementById('next-it').addEventListener('click', () => {
      currentIndexIT = (currentIndexIT + 1) % conversations_it.length;
      updateDialogDemo('chat-history-it', conversations_it, currentIndexIT);
    });

    // Event listeners for TT conversation navigation
    document.getElementById('prev-tt').addEventListener('click', () => {
      currentIndexTT = (currentIndexTT - 1 + conversations_tt.length) % conversations_tt.length;
      updateDialogDemo('chat-history-tt', conversations_tt, currentIndexTT);
    });

    document.getElementById('next-tt').addEventListener('click', () => {
      currentIndexTT = (currentIndexTT + 1) % conversations_tt.length;
      updateDialogDemo('chat-history-tt', conversations_tt, currentIndexTT);
    });
  </script>



  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
